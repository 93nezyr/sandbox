% ドキュメントクラスの定義
\documentclass[twocolumn, a4j, 10pt, fleqn]{ltjsarticle}

\usepackage{enumitem}
\usepackage{url}
\usepackage{amsmath}
\usepackage[margin=10truemm]{geometry}
\usepackage{graphicx}
\usepackage{caption}

% tcolorbox関連
\usepackage{tcolorbox}
\tcbuselibrary{breakable, skins, theorems}

\captionsetup[figure]{format=plain, labelformat=simple, labelsep=period}

\setlength{\columnseprule}{0.1pt}
% \setlength{\mathindent}{0pt}
\renewcommand{\baselinestretch}{0.8}
\renewcommand{\figurename}{Fig}
\setlength{\textfloatsep}{0pt}

% 部名の変更
\renewcommand{\prepartname}{第}
\renewcommand{\postpartname}{部}
\renewcommand{\thepart}{\arabic{part}}

% 本文開始
\begin{document}

% タイトル設定
\title{Neural Network Loss}
\author{93 san}
\maketitle

% 部・節・項・目
\part{ModelX2}
本部では、各節で説明される問題を解決したNeuralNetworkを提案する.

% 節
\section{DQN-MC loss改善}
DQN-MCでは、学習時にモデルが収束しない時に起こっていると考えられる現象として、以下の二つがあげられる。NeuralNetwork自身は、与えられたサンプルに対して十分に学習が進んでおり、これ以上lossが下がる能力がない状況を前提とする.

\vskip\baselineskip
\noindent
\begin{tcolorbox}[
  colback = white,
  colframe = black,
  fonttitle = \bfseries,
  % enlarge top by = -1em,
  % enlarge bottom by = -4em,
  breakable = true]
\begin{itemize}
  \item $Q(s_t, a_1)$と$Q(s_t, a_2)$の真の期待値が十分近い場合、各NN更新ステップのミニバッチ学習時に、ミニバッチ内のターゲットの分布の偏りによって、NNが学習している$Q(s_t, a_1)$と$Q(s_t, a_2)$の大小関係が逆転することで、生成されるエピソードが変動する.\label{item:r1}
  \item NNの精度が十分でない場合、本来十分離れているはずの$Q(s_t, a_1)$と$Q(s_t, a_2)$の真の期待値を十分に予測できず、各NN更新ステップのミニバッチ学習時に、$Q(s_t, a_1)$に対する予測の振動幅と、$Q(s_t, a_2)$に対する予測の振動幅が重なることで、$Q(s_t, a_1)$と$Q(s_t, a_2)$の大小関係が逆転し、生成されるエピソードが変動する.\label{item:r2}
\end{itemize}
\end{tcolorbox}
\noindent
*ただし、$ReplayBuffer$内の各$(s, a)$の組に対するターゲットの期待値が変動しない仮定において.\\
\noindent
*ここで「収束しない」とは、学習中のモデルがgreedyに行動決定した場合でも、生成されるエピソードが変動することを指す.

\vskip\baselineskip
上記の原因二つに対して、改善可能と考えられるのは以下である.

\begin{enumerate}
  \item 報酬関数の変更:\\
        $Q(s_t, a_1)$と$Q(s_t, a_2)$の期待値の差を広げることで、大小関係の逆転を防ぐ. もしくは、完全に各$(s_t, a_k)$に対して、一意の報酬が与えられるような報酬体系にする(DQN-MCでは不可).
  \item NNのloss改善:\label{enum:NNloss}\\
        $Q(s_t, a_1)$と$Q(s_t, a_2)$の期待値が十分に離れていることを前提に、両者の大小関係が逆転しないレベルまでNNのlossを低下させる.
  \item Backup方法の変更:\\
        DQN-MC以外のBackup方法に変更することで、期待値を予測するという学習をやめる.
\end{enumerate}
本節では、上記の\ref{enum:NNloss}について取り組む.

% 項
\subsection{Neural Network Architecture For Decrease loss of ModelX2}
本項では、NeuralNetworkの損失低下について取り組む.
\vskip\baselineskip

% 目
\subsubsection{Ensemble Quantile Regression DQN-MC}
本セクションでは、DQN-MCが期待値を真の目標値として学習することから生じる\ref{item:r1}と、広範で非連続的な状態行動対とターゲットを学習することから生じる\ref{item:r2}を同時に解決する手法を提案する.

まず、NeuralNetworkの更新方法をQuantile Regressionに変更する. これによって真の期待値は近しいが、本来異なるターゲット分布を持つ$Q(s_t, a_1)$と$Q(s_t, a_2)$の行動選択時の関係逆転を防ぐ.

加えて、エージェントが保持するNeuralNetworkを$action space \times N$個にする. ここで、$action$は強化学習における行動を表し、$action space$は時刻$t$でエージェントが実行可能な行動の総数を表す. 本案では、各$action$に対して$N$個のNNが対応することになる. $N$個のNNは状態$s_t$のあるパラメータ$x_{s_t}$をもとに$N$分割された状態空間のみを入力に学習する. 例として、状態$s_t$がパラメータ$x$を持つとした場合、$0 \le x \le 10$の状態は$NN_1$が、$11 \le x \le 20$の状態は$NN_2$が、といったように分割する. これにより広範な状態$s_t$を全て一つの$NN$で学習する必要が無くなり、全体的な$loss$の低減に繋がることを期待する.

ただし、学習中$x$の最大最小値は変動するため、状態の分割方法にも工夫が必要である.

% 加えてNNを$N$個用意する。ミニバッチ取得時に、ターゲットの値をランキング形式に保存するキャッシュを実装し、ターゲットの値が上位$0~1/N \%$に入るものはNN$1$に学習させ、上位$1/N~2/N \%$に入るものはNN$2$に学習させ...というように、各NNに学習させるターゲットの範囲を限定する.

\part{ModelX3}

\end{document}